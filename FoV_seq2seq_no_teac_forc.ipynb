{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_video_ind = ', '098', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '099', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '214', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '215', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '212', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '213', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '210', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '211', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '090', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '092', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '093', 'per_video_db.shape = ', (68, 10, 90))\n",
      "video 094 only has 17 seconds. skip...\n",
      "('_video_ind = ', '095', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '096', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '097', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '133', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '011', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 012 only has 18 seconds. skip...\n",
      "('_video_ind = ', '130', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '014', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '015', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '016', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '134', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '018', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '139', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '138', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '025', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '122', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '026', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '021', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '125', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '023', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '127', 'per_video_db.shape = ', (238, 10, 90))\n",
      "('_video_ind = ', '129', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '029', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 010 only has 18 seconds. skip...\n",
      "('_video_ind = ', '132', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '131', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '013', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '137', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '199', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '135', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '200', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '195', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '194', 'per_video_db.shape = ', (31, 10, 90))\n",
      "video 197 only has 18 seconds. skip...\n",
      "('_video_ind = ', '017', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '191', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '089', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '193', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '115', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '114', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '117', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '116', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '111', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '110', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '113', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '112', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '033', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '030', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '119', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '056', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '034', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '035', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '057', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '173', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '172', 'per_video_db.shape = ', (128, 10, 90))\n",
      "('_video_ind = ', '052', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '053', 'per_video_db.shape = ', (120, 10, 90))\n",
      "video 109 only has 17 seconds. skip...\n",
      "('_video_ind = ', '085', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '049', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '048', 'per_video_db.shape = ', (30, 10, 90))\n",
      "video 102 only has 11 seconds. skip...\n",
      "('_video_ind = ', '046', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '045', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '044', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '106', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '042', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '104', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '105', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '177', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '076', 'per_video_db.shape = ', (238, 10, 90))\n",
      "video 077 only has 19 seconds. skip...\n",
      "('_video_ind = ', '176', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '144', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '152', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '059', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '179', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '178', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '054', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '055', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '175', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '174', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '050', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '051', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '171', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '198', 'per_video_db.shape = ', (87, 10, 90))\n",
      "video 070 only has 19 seconds. skip...\n",
      "('_video_ind = ', '071', 'per_video_db.shape = ', (150, 10, 90))\n",
      "('_video_ind = ', '145', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '182', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '183', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '180', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '181', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '186', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '041', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '184', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '188', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '040', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '187', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '060', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '063', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '196', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '065', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '064', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '067', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '066', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '069', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '068', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '168', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '164', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '165', 'per_video_db.shape = ', (96, 10, 90))\n",
      "video 166 only has 19 seconds. skip...\n",
      "('_video_ind = ', '167', 'per_video_db.shape = ', (32, 10, 90))\n",
      "video 162 only has 19 seconds. skip...\n",
      "('_video_ind = ', '163', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '189', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '038', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '039', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '151', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '150', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '074', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '075', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '072', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '073', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '157', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '156', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '159', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '078', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '062', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '100', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '037', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '147', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '203', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '202', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '204', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '140', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '206', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '082', 'per_video_db.shape = ', (170, 10, 90))\n",
      "('_video_ind = ', '081', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '080', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '087', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '148', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '149', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '003', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '002', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 007 only has 18 seconds. skip...\n",
      "('_video_ind = ', '006', 'per_video_db.shape = ', (132, 10, 90))\n",
      "('_video_ind = ', '004', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '009', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '008', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '120', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '027', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '126', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '020', 'per_video_db.shape = ', (66, 10, 90))\n",
      "Train on 9100 samples, validate on 1012 samples\n",
      "Epoch 1/200\n",
      "9100/9100 [==============================] - 16s 2ms/step - loss: 0.1130 - val_loss: 0.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_4/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_4/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_5/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_5/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_6/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_6/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_7/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_7/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_8/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_8/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "9100/9100 [==============================] - 7s 813us/step - loss: 0.1069 - val_loss: 0.1208\n",
      "Epoch 3/200\n",
      " 160/9100 [..............................] - ETA: 7s - loss: 0.1075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_4/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_4/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_5/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_5/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_6/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_6/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_7/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_7/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_8/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_8/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9100/9100 [==============================] - 7s 818us/step - loss: 0.1052 - val_loss: 0.1187\n",
      "Epoch 4/200\n",
      " 160/9100 [..............................] - ETA: 7s - loss: 0.1171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_4/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_4/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_5/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_5/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_6/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_6/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_7/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_7/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_8/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_8/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9100/9100 [==============================] - 8s 830us/step - loss: 0.1038 - val_loss: 0.1189\n",
      "Epoch 5/200\n",
      "9100/9100 [==============================] - 7s 814us/step - loss: 0.1031 - val_loss: 0.1181\n",
      "Epoch 6/200\n",
      "  96/9100 [..............................] - ETA: 8s - loss: 0.1001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_4/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_4/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_5/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_5/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_6/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_6/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_7/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_7/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_8/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_8/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9100/9100 [==============================] - 8s 825us/step - loss: 0.1024 - val_loss: 0.1208\n",
      "Epoch 7/200\n",
      "9100/9100 [==============================] - 7s 810us/step - loss: 0.1018 - val_loss: 0.1155\n",
      "Epoch 8/200\n",
      " 160/9100 [..............................] - ETA: 7s - loss: 0.0950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_1/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_1/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_2/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_2/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_3/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_3/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_4/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_4/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_5/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_5/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_6/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_6/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_7/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_7/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/Users/maojuntao/anaconda3/lib/python2.7/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_2_8/while/Exit_3:0' shape=(?, 64) dtype=float32>, <tf.Tensor 'lstm_2_8/while/Exit_4:0' shape=(?, 64) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9100/9100 [==============================] - 7s 823us/step - loss: 0.1008 - val_loss: 0.1170\n",
      "Epoch 9/200\n",
      "9100/9100 [==============================] - 7s 802us/step - loss: 0.1003 - val_loss: 0.1193\n",
      "Epoch 10/200\n",
      "9100/9100 [==============================] - 7s 819us/step - loss: 0.0994 - val_loss: 0.1183\n",
      "Epoch 11/200\n",
      "9100/9100 [==============================] - 8s 832us/step - loss: 0.0969 - val_loss: 0.1181\n",
      "Epoch 12/200\n",
      "9100/9100 [==============================] - 7s 806us/step - loss: 0.0962 - val_loss: 0.1183\n",
      "Epoch 13/200\n",
      "9100/9100 [==============================] - 8s 832us/step - loss: 0.0959 - val_loss: 0.1189\n",
      "Epoch 14/200\n",
      "9100/9100 [==============================] - 8s 831us/step - loss: 0.0951 - val_loss: 0.1180\n",
      "Epoch 15/200\n",
      "9100/9100 [==============================] - 7s 813us/step - loss: 0.0950 - val_loss: 0.1182\n",
      "Epoch 16/200\n",
      "9100/9100 [==============================] - 7s 806us/step - loss: 0.0949 - val_loss: 0.1178\n",
      "Epoch 17/200\n",
      "9100/9100 [==============================] - 7s 819us/step - loss: 0.0947 - val_loss: 0.1180\n",
      "('_video_ind = ', '146', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '153', 'per_video_db.shape = ', (96, 10, 90))\n",
      "video 155 only has 19 seconds. skip...\n",
      "('_video_ind = ', '154', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '158', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '190', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '079', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '128', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '058', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '136', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '032', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '019', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '031', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '118', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '170', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '160', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '028', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '209', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '185', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '201', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '088', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '142', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '143', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '141', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '083', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '208', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '192', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '024', 'per_video_db.shape = ', (132, 10, 90))\n",
      "video 001 only has 16 seconds. skip...\n",
      "('_video_ind = ', '123', 'per_video_db.shape = ', (68, 10, 90))\n",
      "video 124 only has 14 seconds. skip...\n",
      "('_video_ind = ', '169', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '005', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '022', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '047', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '103', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '101', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '043', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '161', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '205', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '091', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '121', 'per_video_db.shape = ', (68, 10, 90))\n",
      "Testing finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n##################################################################################################################\\n\\n    ### ==============Below is concating with others' future branch convLSTM===========================\\n    ## utility layers\\n    flatten_layer = Flatten()\\n    expand_dim_layer = Lambda(lambda x: K.expand_dims(x,1))\\n    Concatenatelayer = Concatenate(axis=1)\\n    get_dim1_layer = Lambda(lambda x: x[:,0,:])\\n\\n\\n    ### ====================Graph def====================\\n    # The first part is unchanged\\n    encoder_inputs = Input(shape=(None, num_encoder_tokens))\\n    encoder = LSTM(latent_dim, return_state=True)\\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\\n    states = [state_h, state_c]\\n\\n\\n    ###======convLSTM on others' future======\\n    other_fut_lstm = ConvLSTM2D(filters=latent_dim, kernel_size=(num_user-1, 3),\\n                       input_shape=(1, num_user-1, fps, 3),\\n                       padding='same', return_sequences=True, return_state=True)\\n    others_fut_inputs = Input(shape=(max_decoder_seq_length,num_user-1,fps,3))\\n    # others_fut_inputs = Input(batch_shape=(batch_size,1,num_user-1,fps,3))\\n    flatten_conv_lstm_state_dense = Dense(latent_dim)\\n\\n    fut_outputs_sqns, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\\n\\n\\n\\n\\n    # Set up the decoder, which will only process one timestep at a time.\\n    decoder_inputs = Input(shape=(1, num_decoder_tokens))\\n    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\\n    # decoder_inputs = Input(batch_shape=(batch_size, 1, num_decoder_tokens))\\n    # decoder_lstm = LSTM(latent_dim, return_sequences=False, return_state=True, stateful=True)\\n    decoder_dense = Dense(num_decoder_tokens,activation='tanh')\\n\\n\\n    ## concat states\\n    all_outputs = []\\n    inputs = decoder_inputs\\n    for time_ind in range(max_decoder_seq_length):\\n        # Run the decoder on one timestep\\n        decoder_states, state_h, state_c = decoder_lstm(inputs,initial_state=states)\\n\\n        # ### caution: it seems keras convLSTM is by default stateful, don't have to feed back last hidden states.\\n        # ### is this true?\\n        # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\\n        # # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs,initial_state=others_states)#erros?!!!\\n        \\n        # fut_outputs = identity_layer(fut_outputs_sqns[:,time_ind,:,:,:])\\n        fut_outputs = slice_layer(1,time_ind,time_ind+1)(fut_outputs_sqns)\\n        convlstm_state = flatten_layer(fut_outputs)\\n        convlstm_state = flatten_conv_lstm_state_dense(convlstm_state)\\n        concat_state = Concatenatelayer([get_dim1_layer(decoder_states),convlstm_state])\\n        outputs = decoder_dense(concat_state)\\n        outputs = expand_dim_layer(outputs)\\n        all_outputs.append(outputs)\\n\\n        inputs = outputs\\n        states = [state_h, state_c]\\n\\n        # others_fut_inputs = others_fut_inputs #TODO feed gt others for next step\\n        # others_states = [others_state_h, others_state_c]\\n\\n\\n    # Concatenate all predictions\\n    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\\n\\n\\n    # Define and compile model as previously\\n    model = Model([encoder_inputs, others_fut_inputs, decoder_inputs], decoder_outputs)\\n    model.compile(optimizer='Adam', loss='mean_squared_error')\\n\\n\\n\\n\\n    #### ====================data====================\\n    # load cached data\\n    _video_db_tar = pickle.load(open('./cache/_video_db_tar_exp'+str(experiment)+'.p','rb'))\\n    _video_db_future_tar = pickle.load(open('./cache/_video_db_future_tar_exp'+str(experiment)+'.p','rb'))\\n    _video_db_future_input_tar = pickle.load(open('./cache/_video_db_future_input_tar_exp'+str(experiment)+'.p','rb'))\\n    _video_db_oth = pickle.load(open('./cache/_video_db_oth_exp'+str(experiment)+'.p','rb'))\\n    _video_db_future_oth = pickle.load(open('./cache/_video_db_future_oth_exp'+str(experiment)+'.p','rb'))\\n    _video_db_future_input_oth = pickle.load(open('./cache/_video_db_future_input_oth_exp'+str(experiment)+'.p','rb'))\\n\\n\\n\\n\\n    def data_sanity_check(_video_db_tar,_video_db_future_tar,_video_db_future_input_tar):\\n        sample_ind = np.random.randint(0,_video_db_future_input_tar.shape[0])\\n        assert (_video_db_tar[sample_ind,:][-1,:]-_video_db_future_input_tar[sample_ind,:][0,:]).sum()==0\\n        print(np.abs(_video_db_tar[sample_ind,:][-1,:]-_video_db_future_tar[sample_ind,:][0,:]))\\n        \\n    def _reshape_others_data(_video_db_oth):\\n        ## to match Input shape: others_fut_inputs\\n        _video_db_oth = _video_db_oth.transpose((1,2,0,3))\\n        _video_db_oth = _video_db_oth.reshape((_video_db_oth.shape[0],_video_db_oth.shape[1],_video_db_oth.shape[2],\\n                        fps,3))\\n        return _video_db_oth\\n\\n    _video_db_oth = _reshape_others_data(_video_db_oth)\\n    _video_db_future_oth = _reshape_others_data(_video_db_future_oth)\\n    # _video_db_future_input_oth = _reshape_others_data(_video_db_future_input_oth)\\n    total_num_samples = _video_db_tar.shape[0]\\n\\n    #### shuffle the whole dataset\\n    # index_shuf = get_shuffle_index(total_num_samples)\\n    index_shuf = pickle.load(open('index_shuf'+'_exp'+str(experiment)+'.p','rb'))\\n    print('Shuffle data before training and testing.')\\n    _video_db_tar = shuffle_data(index_shuf,_video_db_tar)\\n    _video_db_future_tar = shuffle_data(index_shuf,_video_db_future_tar)\\n    _video_db_future_input_tar = shuffle_data(index_shuf,_video_db_future_input_tar)\\n\\n    # _video_db_oth = shuffle_data(index_shuf,_video_db_oth)\\n    _video_db_future_oth = shuffle_data(index_shuf,_video_db_future_oth)\\n    # _video_db_future_input_oth = shuffle_data(index_shuf,_video_db_future_input_oth)\\n\\n\\n    #### prepare training data\\n    data_sanity_check(_video_db_tar,_video_db_future_tar,_video_db_future_input_tar)\\n    num_testing_sample = int(0.15*total_num_samples)#use last few as test\\n    encoder_input_data = _video_db_tar[:-num_testing_sample,:,:]\\n    decoder_target_data = get_gt_target_xyz(_video_db_future_tar)[:-num_testing_sample,:,:]\\n    # decoder_input_data = get_gt_target_xyz(_video_db_future_input_tar)[:-num_testing_sample,-1,:][:,np.newaxis,:]\\n    decoder_input_data = get_gt_target_xyz(_video_db_future_input_tar)[:-num_testing_sample,0,:][:,np.newaxis,:]\\n\\n    others_fut_input_data = _video_db_future_oth[:-num_testing_sample,:]\\n\\n\\n\\n\\n\\n    ### ====================Training====================\\n    # model = load_model('fov_s2s_notfor_concatConvLSTM_epoch82-0.0168.h5')\\n    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch10-0.0487.h5')\\n    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch06-0.0155.h5')\\n    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch26-0.0096.h5')\\n    model_checkpoint = ModelCheckpoint('concat_future_ConvLSTM_newdata_epoch{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss', save_best_only=True)\\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\\n                                     patience=3, min_lr=1e-6)\\n    stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\\n    # Train model as previously \\n    model.fit([encoder_input_data, others_fut_input_data, decoder_input_data], decoder_target_data,\\n              batch_size=batch_size,\\n              epochs=epochs,\\n              validation_split=0.2,\\n              shuffle=True, initial_epoch=0,\\n              callbacks=[model_checkpoint, reduce_lr, stopping])\\n\\n\\n\\n\\n\\n\\n    ### ====================Testing====================\\n    # model = load_model('fov_s2s_notfor_concatConvLSTM_epoch82-0.0168.h5')\\n    # model = load_model('concat_future_ConvLSTM_newdata_epoch08-0.8951.h5')\\n    def decode_sequence_fov(input_seq,others_fut_input_seq):\\n        # Encode the input as state vectors.\\n        last_location = input_seq[0,-1,:][np.newaxis,np.newaxis,:]\\n        last_mu_var = get_gt_target_xyz(last_location)\\n        decoded_sentence = model.predict([input_seq,others_fut_input_seq,last_mu_var])\\n        return decoded_sentence\\n\\n    gt_sentence_list = []\\n    decoded_sentence_list = []\\n    for seq_index in range(total_num_samples-num_testing_sample,total_num_samples):\\n        input_seq = _video_db_tar[seq_index: seq_index + 1,:,:]\\n        others_fut_input_seq = _video_db_future_oth[seq_index: seq_index + 1,:]\\n        decoded_sentence = decode_sequence_fov(input_seq,others_fut_input_seq)\\n        decoded_sentence_list+=[decoded_sentence]\\n        gt_sentence = _video_db_future_tar[seq_index: seq_index + 1,:,:]\\n        gt_sentence_list+=[gt_sentence]\\n        # print('-')\\n        # decoder_target = get_gt_target_xyz(gt_sentence)\\n        # print('Decoded sentence - decoder_target:', np.squeeze(np.array(decoded_sentence))[:,:3]-np.squeeze(decoder_target)[:,:3])\\n\\n    pickle.dump(decoded_sentence_list,open('decoded_sentence.p','wb'))\\n    pickle.dump(gt_sentence_list,open('gt_sentence_list.p','wb'))\\n    print('Testing finished!')\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    ####TimeDistributed\\n\\n    # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\\n    # # convlstm_state = flatten_conv_lstm_state_dense(K.reshape(fut_outputs,(-1,max_decoder_seq_length,(num_user-1)*fps*3)))\\n\\n    # td_layer = TimeDistributed(flatten_conv_lstm_state_dense)\\n    # # convlstm_state = td_layer(K.reshape(fut_outputs,(-1,max_decoder_seq_length,(num_user-1)*fps*3)))\\n    # convlstm_state = flatten_layer(fut_outputs)\\n\\n\\n\\n    ####!!check compatibility, EVERY layer needs to be keras tensor (especially concat, slice, backend ops)\\n\\n    # model1 = Model(encoder_inputs, encoder_outputs)\\n\\n    # model2 = Model(others_fut_inputs, convlstm_state)\\n    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], concat_state)\\n    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], outputs)\\n    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], decoder_outputs)\\n\\n\\n    # model3 = Model([decoder_inputs,encoder_inputs], decoder_states)\\n    # model3 = Model([decoder_inputs,encoder_inputs], state_h)\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "-first part:\n",
    "seq2seq without teacher forcing\n",
    "-second part:\n",
    "seq2seq without teacher forcing, with others' future convLSTM\n",
    "concat states with decoder LSTM and then predict\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers import Lambda,Concatenate,Flatten,ConvLSTM2D\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import sys,glob,io,random\n",
    "if '/Users/maojuntao/Downloads/project/' not in sys.path:\n",
    "    sys.path.insert(0, '/Users/maojuntao/Downloads/project/')\n",
    "from dataLayer import DataLayer\n",
    "import cost as costfunc\n",
    "from config import cfg\n",
    "from dataIO import clip_xyz\n",
    "from utility import reshape2second_stacks,get_data\n",
    "from utility import get_shuffle_index,shuffle_data,get_gt_target_xyz,get_gt_target_xyz_oth\n",
    "from utility import slice_layer\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "### ====================Graph def====================\n",
    "def onelayer_tar_seq2seq():\n",
    "    # The first part is unchanged\n",
    "    if not cfg.input_mean_var:\n",
    "        encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(None, num_decoder_tokens))    \n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, which will only process one timestep at a time.\n",
    "    decoder_inputs = Input(shape=(1, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_dense = Dense(num_decoder_tokens,activation='tanh')\n",
    "\n",
    "    all_outputs = []\n",
    "    inputs = decoder_inputs\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        # Run the decoder on one timestep\n",
    "        decoder_states, state_h, state_c = decoder_lstm(inputs,\n",
    "                                                 initial_state=states)\n",
    "        outputs = decoder_dense(decoder_states)\n",
    "        # Store the current prediction (we will concatenate all predictions later)\n",
    "        all_outputs.append(outputs)\n",
    "        # Reinject the outputs as inputs for the next loop iteration\n",
    "        # as well as update the states\n",
    "        inputs = outputs\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "    # Define and compile model as previously\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #experiment = 1\n",
    "    batch_size = 32  # Batch size for training.\n",
    "    epochs = 200  # Number of epochs to train for.\n",
    "    latent_dim = 64  # Latent dimensionality of the encoding space.\n",
    "\n",
    "    fps = 30\n",
    "    num_encoder_tokens = 3*fps\n",
    "    num_decoder_tokens = 6\n",
    "    max_encoder_seq_length = cfg.running_length\n",
    "    max_decoder_seq_length = cfg.predict_step\n",
    "    # num_user = 48\n",
    "\n",
    "\n",
    "    model = onelayer_tar_seq2seq()\n",
    "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "    #### ====================data====================\n",
    "    ## load data just as in Fov_seq2seq.py\n",
    "    # if cfg.use_xyz:\n",
    "    #     all_video_data = pickle.load(open('/scratch/cl2840/360video/data/new_exp_'+str(experiment)+'_xyz.p','rb'))\n",
    "    #     data_dim = 3\n",
    "    # all_video_data = clip_xyz(all_video_data)\n",
    "    # datadb = all_video_data.copy()\n",
    "    ##### data format 3or4--4\n",
    "    # video_data_train = pickle.load(open('/scratch/cl2840/360video/data/shanghai_dataset_xyz_train.p','rb'))    \n",
    "    #### format5\n",
    "    video_data_train = pickle.load(open('/Users/maojuntao/Downloads/project/data 2/shanghai_dataset_xyz_train.p','rb')) \n",
    "    video_data_train = clip_xyz(video_data_train)\n",
    "    datadb = video_data_train.copy()\n",
    "\n",
    "\n",
    "    _video_db,_video_db_future,_video_db_future_input = get_data(datadb,pick_user=False)\n",
    "    # total_num_samples = _video_db.shape[0]\n",
    "\n",
    "    if cfg.shuffle_data:\n",
    "        #shuffle the whole dataset\n",
    "        # index_shuf = get_shuffle_index(total_num_samples)\n",
    "        index_shuf = pickle.load(open('index_shuf'+'_exp'+str(experiment)+'.p','rb'))\n",
    "        _video_db = shuffle_data(index_shuf,_video_db)\n",
    "        _video_db_future = shuffle_data(index_shuf,_video_db_future)\n",
    "        _video_db_future_input = shuffle_data(index_shuf,_video_db_future_input)\n",
    "\n",
    "\n",
    "    # num_testing_sample = int(0.15*total_num_samples)#use last few as test\n",
    "    num_testing_sample = 1\n",
    "    if cfg.input_mean_var:\n",
    "        encoder_input_data = get_gt_target_xyz(_video_db[:-num_testing_sample,:,:])\n",
    "    else:\n",
    "        encoder_input_data = _video_db[:-num_testing_sample,:,:]\n",
    "    decoder_target_data = get_gt_target_xyz(_video_db_future)[:-num_testing_sample,:,:]\n",
    "    # decoder_input_data = get_gt_target_xyz(_video_db_future_input)[:-num_testing_sample,-1,:][:,np.newaxis,:]\n",
    "    decoder_input_data = get_gt_target_xyz(_video_db_future_input)[:-num_testing_sample,0,:][:,np.newaxis,:]\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Training====================\n",
    "    # model = load_model('fov_s2s_noteacherforcing_epoch21-0.0759.h5')\n",
    "    # tag = 'notFor_tanh_newdata_exp2_epoch'\n",
    "    # tag = 'fctar_seqseq_shanghai_traintest_split_predmeanvar_Aug9'\n",
    "    # tag = 'fctar_seqseq_shanghai_traintest_split_meanvarmeanvar_Aug9'\n",
    "    tag = 'fctar_seqseq_THU_predmeanvar_Sep5'\n",
    "    model_checkpoint = ModelCheckpoint(tag+'{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                     patience=3, min_lr=1e-6)\n",
    "    stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True,initial_epoch=0,\n",
    "              callbacks=[model_checkpoint, reduce_lr, stopping])\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Testing====================\n",
    "    ##### data format 3or4--4\n",
    "    # video_data_test = pickle.load(open('/scratch/cl2840/360video/data/shanghai_dataset_xyz_test.p','rb'))\n",
    "    ### data format 5\n",
    "    video_data_test = pickle.load(open('/Users/maojuntao/Downloads/project/data 2/shanghai_dataset_xyz_test.p','rb'))\n",
    "\n",
    "    video_data_test = clip_xyz(video_data_test)\n",
    "    datadb = video_data_test.copy()   ##arrange the test data\n",
    "    _video_db,_video_db_future,_video_db_future_input = get_data(datadb,pick_user=False)\n",
    "\n",
    "    if cfg.input_mean_var:\n",
    "        _video_db = get_gt_target_xyz(_video_db)\n",
    "\n",
    "    # model = load_model('fov_s2s_noteacherforcing.h5')\n",
    "    # model = load_model('fov_s2s_noteacherforcing_epoch74-0.0087.h5')\n",
    "    # model = load_model('notFor_tanh_newdata_epoch18-0.0761.h5')\n",
    "    # model = load_model('notFor_tanh_newdata_exp2_epoch200-0.0302.h5')\n",
    "    def decode_sequence_fov(input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        last_location = input_seq[0,-1,:][np.newaxis,np.newaxis,:]\n",
    "        if not cfg.input_mean_var:\n",
    "            last_mu_var = get_gt_target_xyz(last_location)\n",
    "        else:\n",
    "            last_mu_var = last_location\n",
    "        decoded_sentence = model.predict([input_seq,last_mu_var])\n",
    "        return decoded_sentence\n",
    "\n",
    "\n",
    "    gt_sentence_list = []\n",
    "    decoded_sentence_list = []\n",
    "    # for seq_index in range(total_num_samples-num_testing_sample,total_num_samples):\n",
    "    for seq_index in range(_video_db.shape[0]):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = _video_db[seq_index: seq_index + 1,:,:]\n",
    "\n",
    "        decoded_sentence = decode_sequence_fov(input_seq)\n",
    "        decoded_sentence_list+=[decoded_sentence]\n",
    "        gt_sentence = _video_db_future[seq_index: seq_index + 1,:,:]\n",
    "        gt_sentence_list+=[gt_sentence]\n",
    "        decoder_target = get_gt_target_xyz(gt_sentence)\n",
    "        # print('-')\n",
    "        # print('Decoded sentence - decoder_target:', np.squeeze(np.array(decoded_sentence))[:,:3]-np.squeeze(decoder_target)[:,:3])\n",
    "\n",
    "    pickle.dump(decoded_sentence_list,open('decoded_sentence'+tag+'.p','wb'))\n",
    "    pickle.dump(gt_sentence_list,open('gt_sentence_list'+tag+'.p','wb'))\n",
    "    print('Testing finished!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "##################################################################################################################\n",
    "\n",
    "    ### ==============Below is concating with others' future branch convLSTM===========================\n",
    "    ## utility layers\n",
    "    flatten_layer = Flatten()\n",
    "    expand_dim_layer = Lambda(lambda x: K.expand_dims(x,1))\n",
    "    Concatenatelayer = Concatenate(axis=1)\n",
    "    get_dim1_layer = Lambda(lambda x: x[:,0,:])\n",
    "\n",
    "\n",
    "    ### ====================Graph def====================\n",
    "    # The first part is unchanged\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    states = [state_h, state_c]\n",
    "\n",
    "\n",
    "    ###======convLSTM on others' future======\n",
    "    other_fut_lstm = ConvLSTM2D(filters=latent_dim, kernel_size=(num_user-1, 3),\n",
    "                       input_shape=(1, num_user-1, fps, 3),\n",
    "                       padding='same', return_sequences=True, return_state=True)\n",
    "    others_fut_inputs = Input(shape=(max_decoder_seq_length,num_user-1,fps,3))\n",
    "    # others_fut_inputs = Input(batch_shape=(batch_size,1,num_user-1,fps,3))\n",
    "    flatten_conv_lstm_state_dense = Dense(latent_dim)\n",
    "\n",
    "    fut_outputs_sqns, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Set up the decoder, which will only process one timestep at a time.\n",
    "    decoder_inputs = Input(shape=(1, num_decoder_tokens))\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    # decoder_inputs = Input(batch_shape=(batch_size, 1, num_decoder_tokens))\n",
    "    # decoder_lstm = LSTM(latent_dim, return_sequences=False, return_state=True, stateful=True)\n",
    "    decoder_dense = Dense(num_decoder_tokens,activation='tanh')\n",
    "\n",
    "\n",
    "    ## concat states\n",
    "    all_outputs = []\n",
    "    inputs = decoder_inputs\n",
    "    for time_ind in range(max_decoder_seq_length):\n",
    "        # Run the decoder on one timestep\n",
    "        decoder_states, state_h, state_c = decoder_lstm(inputs,initial_state=states)\n",
    "\n",
    "        # ### caution: it seems keras convLSTM is by default stateful, don't have to feed back last hidden states.\n",
    "        # ### is this true?\n",
    "        # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\n",
    "        # # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs,initial_state=others_states)#erros?!!!\n",
    "        \n",
    "        # fut_outputs = identity_layer(fut_outputs_sqns[:,time_ind,:,:,:])\n",
    "        fut_outputs = slice_layer(1,time_ind,time_ind+1)(fut_outputs_sqns)\n",
    "        convlstm_state = flatten_layer(fut_outputs)\n",
    "        convlstm_state = flatten_conv_lstm_state_dense(convlstm_state)\n",
    "        concat_state = Concatenatelayer([get_dim1_layer(decoder_states),convlstm_state])\n",
    "        outputs = decoder_dense(concat_state)\n",
    "        outputs = expand_dim_layer(outputs)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "        inputs = outputs\n",
    "        states = [state_h, state_c]\n",
    "\n",
    "        # others_fut_inputs = others_fut_inputs #TODO feed gt others for next step\n",
    "        # others_states = [others_state_h, others_state_c]\n",
    "\n",
    "\n",
    "    # Concatenate all predictions\n",
    "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
    "\n",
    "\n",
    "    # Define and compile model as previously\n",
    "    model = Model([encoder_inputs, others_fut_inputs, decoder_inputs], decoder_outputs)\n",
    "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### ====================data====================\n",
    "    # load cached data\n",
    "    _video_db_tar = pickle.load(open('./cache/_video_db_tar_exp'+str(experiment)+'.p','rb'))\n",
    "    _video_db_future_tar = pickle.load(open('./cache/_video_db_future_tar_exp'+str(experiment)+'.p','rb'))\n",
    "    _video_db_future_input_tar = pickle.load(open('./cache/_video_db_future_input_tar_exp'+str(experiment)+'.p','rb'))\n",
    "    _video_db_oth = pickle.load(open('./cache/_video_db_oth_exp'+str(experiment)+'.p','rb'))\n",
    "    _video_db_future_oth = pickle.load(open('./cache/_video_db_future_oth_exp'+str(experiment)+'.p','rb'))\n",
    "    _video_db_future_input_oth = pickle.load(open('./cache/_video_db_future_input_oth_exp'+str(experiment)+'.p','rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def data_sanity_check(_video_db_tar,_video_db_future_tar,_video_db_future_input_tar):\n",
    "        sample_ind = np.random.randint(0,_video_db_future_input_tar.shape[0])\n",
    "        assert (_video_db_tar[sample_ind,:][-1,:]-_video_db_future_input_tar[sample_ind,:][0,:]).sum()==0\n",
    "        print(np.abs(_video_db_tar[sample_ind,:][-1,:]-_video_db_future_tar[sample_ind,:][0,:]))\n",
    "        \n",
    "    def _reshape_others_data(_video_db_oth):\n",
    "        ## to match Input shape: others_fut_inputs\n",
    "        _video_db_oth = _video_db_oth.transpose((1,2,0,3))\n",
    "        _video_db_oth = _video_db_oth.reshape((_video_db_oth.shape[0],_video_db_oth.shape[1],_video_db_oth.shape[2],\n",
    "                        fps,3))\n",
    "        return _video_db_oth\n",
    "\n",
    "    _video_db_oth = _reshape_others_data(_video_db_oth)\n",
    "    _video_db_future_oth = _reshape_others_data(_video_db_future_oth)\n",
    "    # _video_db_future_input_oth = _reshape_others_data(_video_db_future_input_oth)\n",
    "    total_num_samples = _video_db_tar.shape[0]\n",
    "\n",
    "    #### shuffle the whole dataset\n",
    "    # index_shuf = get_shuffle_index(total_num_samples)\n",
    "    index_shuf = pickle.load(open('index_shuf'+'_exp'+str(experiment)+'.p','rb'))\n",
    "    print('Shuffle data before training and testing.')\n",
    "    _video_db_tar = shuffle_data(index_shuf,_video_db_tar)\n",
    "    _video_db_future_tar = shuffle_data(index_shuf,_video_db_future_tar)\n",
    "    _video_db_future_input_tar = shuffle_data(index_shuf,_video_db_future_input_tar)\n",
    "\n",
    "    # _video_db_oth = shuffle_data(index_shuf,_video_db_oth)\n",
    "    _video_db_future_oth = shuffle_data(index_shuf,_video_db_future_oth)\n",
    "    # _video_db_future_input_oth = shuffle_data(index_shuf,_video_db_future_input_oth)\n",
    "\n",
    "\n",
    "    #### prepare training data\n",
    "    data_sanity_check(_video_db_tar,_video_db_future_tar,_video_db_future_input_tar)\n",
    "    num_testing_sample = int(0.15*total_num_samples)#use last few as test\n",
    "    encoder_input_data = _video_db_tar[:-num_testing_sample,:,:]\n",
    "    decoder_target_data = get_gt_target_xyz(_video_db_future_tar)[:-num_testing_sample,:,:]\n",
    "    # decoder_input_data = get_gt_target_xyz(_video_db_future_input_tar)[:-num_testing_sample,-1,:][:,np.newaxis,:]\n",
    "    decoder_input_data = get_gt_target_xyz(_video_db_future_input_tar)[:-num_testing_sample,0,:][:,np.newaxis,:]\n",
    "\n",
    "    others_fut_input_data = _video_db_future_oth[:-num_testing_sample,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Training====================\n",
    "    # model = load_model('fov_s2s_notfor_concatConvLSTM_epoch82-0.0168.h5')\n",
    "    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch10-0.0487.h5')\n",
    "    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch06-0.0155.h5')\n",
    "    # model = load_model('fov_s2s_noTfor_concatConvLSTM_48user_shuffle_epoch26-0.0096.h5')\n",
    "    model_checkpoint = ModelCheckpoint('concat_future_ConvLSTM_newdata_epoch{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                     patience=3, min_lr=1e-6)\n",
    "    stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "    # Train model as previously \n",
    "    model.fit([encoder_input_data, others_fut_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2,\n",
    "              shuffle=True, initial_epoch=0,\n",
    "              callbacks=[model_checkpoint, reduce_lr, stopping])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Testing====================\n",
    "    # model = load_model('fov_s2s_notfor_concatConvLSTM_epoch82-0.0168.h5')\n",
    "    # model = load_model('concat_future_ConvLSTM_newdata_epoch08-0.8951.h5')\n",
    "    def decode_sequence_fov(input_seq,others_fut_input_seq):\n",
    "        # Encode the input as state vectors.\n",
    "        last_location = input_seq[0,-1,:][np.newaxis,np.newaxis,:]\n",
    "        last_mu_var = get_gt_target_xyz(last_location)\n",
    "        decoded_sentence = model.predict([input_seq,others_fut_input_seq,last_mu_var])\n",
    "        return decoded_sentence\n",
    "\n",
    "    gt_sentence_list = []\n",
    "    decoded_sentence_list = []\n",
    "    for seq_index in range(total_num_samples-num_testing_sample,total_num_samples):\n",
    "        input_seq = _video_db_tar[seq_index: seq_index + 1,:,:]\n",
    "        others_fut_input_seq = _video_db_future_oth[seq_index: seq_index + 1,:]\n",
    "        decoded_sentence = decode_sequence_fov(input_seq,others_fut_input_seq)\n",
    "        decoded_sentence_list+=[decoded_sentence]\n",
    "        gt_sentence = _video_db_future_tar[seq_index: seq_index + 1,:,:]\n",
    "        gt_sentence_list+=[gt_sentence]\n",
    "        # print('-')\n",
    "        # decoder_target = get_gt_target_xyz(gt_sentence)\n",
    "        # print('Decoded sentence - decoder_target:', np.squeeze(np.array(decoded_sentence))[:,:3]-np.squeeze(decoder_target)[:,:3])\n",
    "\n",
    "    pickle.dump(decoded_sentence_list,open('decoded_sentence.p','wb'))\n",
    "    pickle.dump(gt_sentence_list,open('gt_sentence_list.p','wb'))\n",
    "    print('Testing finished!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####TimeDistributed\n",
    "\n",
    "    # fut_outputs, others_state_h, others_state_c = other_fut_lstm(others_fut_inputs)\n",
    "    # # convlstm_state = flatten_conv_lstm_state_dense(K.reshape(fut_outputs,(-1,max_decoder_seq_length,(num_user-1)*fps*3)))\n",
    "\n",
    "    # td_layer = TimeDistributed(flatten_conv_lstm_state_dense)\n",
    "    # # convlstm_state = td_layer(K.reshape(fut_outputs,(-1,max_decoder_seq_length,(num_user-1)*fps*3)))\n",
    "    # convlstm_state = flatten_layer(fut_outputs)\n",
    "\n",
    "\n",
    "\n",
    "    ####!!check compatibility, EVERY layer needs to be keras tensor (especially concat, slice, backend ops)\n",
    "\n",
    "    # model1 = Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "    # model2 = Model(others_fut_inputs, convlstm_state)\n",
    "    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], concat_state)\n",
    "    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], outputs)\n",
    "    # model5 = Model([encoder_inputs,others_fut_inputs,decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "    # model3 = Model([decoder_inputs,encoder_inputs], decoder_states)\n",
    "    # model3 = Model([decoder_inputs,encoder_inputs], state_h)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2593, 10, 90)\n"
     ]
    }
   ],
   "source": [
    "print(_video_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 90)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(input_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "print(decoded_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-febdeb33ca21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(decoder_target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "#print(decoder_target)\n",
    "print('',decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_video_ind = ', '098', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '099', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '214', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '215', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '212', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '213', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '210', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '211', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '090', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '092', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '093', 'per_video_db.shape = ', (68, 10, 90))\n",
      "video 094 only has 17 seconds. skip...\n",
      "('_video_ind = ', '095', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '096', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '097', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '133', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '011', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 012 only has 18 seconds. skip...\n",
      "('_video_ind = ', '130', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '014', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '015', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '016', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '134', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '018', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '139', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '138', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '025', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '122', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '026', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '021', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '125', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '023', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '127', 'per_video_db.shape = ', (238, 10, 90))\n",
      "('_video_ind = ', '129', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '029', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 010 only has 18 seconds. skip...\n",
      "('_video_ind = ', '132', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '131', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '013', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '137', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '199', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '135', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '200', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '195', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '194', 'per_video_db.shape = ', (31, 10, 90))\n",
      "video 197 only has 18 seconds. skip...\n",
      "('_video_ind = ', '017', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '191', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '089', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '193', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '115', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '114', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '117', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '116', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '111', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '110', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '113', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '112', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '033', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '030', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '119', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '056', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '034', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '035', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '057', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '173', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '172', 'per_video_db.shape = ', (128, 10, 90))\n",
      "('_video_ind = ', '052', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '053', 'per_video_db.shape = ', (120, 10, 90))\n",
      "video 109 only has 17 seconds. skip...\n",
      "('_video_ind = ', '085', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '049', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '048', 'per_video_db.shape = ', (30, 10, 90))\n",
      "video 102 only has 11 seconds. skip...\n",
      "('_video_ind = ', '046', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '045', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '044', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '106', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '042', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '104', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '105', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '177', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '076', 'per_video_db.shape = ', (238, 10, 90))\n",
      "video 077 only has 19 seconds. skip...\n",
      "('_video_ind = ', '176', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '144', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '152', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '059', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '179', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '178', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '054', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '055', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '175', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '174', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '050', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '051', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '171', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '198', 'per_video_db.shape = ', (87, 10, 90))\n",
      "video 070 only has 19 seconds. skip...\n",
      "('_video_ind = ', '071', 'per_video_db.shape = ', (150, 10, 90))\n",
      "('_video_ind = ', '145', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '182', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '183', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '180', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '181', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '186', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '041', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '184', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '188', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '040', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '187', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '060', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '063', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '196', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '065', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '064', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '067', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '066', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '069', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '068', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '168', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '164', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '165', 'per_video_db.shape = ', (96, 10, 90))\n",
      "video 166 only has 19 seconds. skip...\n",
      "('_video_ind = ', '167', 'per_video_db.shape = ', (32, 10, 90))\n",
      "video 162 only has 19 seconds. skip...\n",
      "('_video_ind = ', '163', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '189', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '038', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '039', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '151', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '150', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '074', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '075', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '072', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '073', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '157', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '156', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '159', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '078', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '062', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '100', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '037', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '147', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '203', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '202', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '204', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '140', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '206', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '082', 'per_video_db.shape = ', (170, 10, 90))\n",
      "('_video_ind = ', '081', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '080', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '087', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '148', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '149', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '003', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '002', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 007 only has 18 seconds. skip...\n",
      "('_video_ind = ', '006', 'per_video_db.shape = ', (132, 10, 90))\n",
      "('_video_ind = ', '004', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '009', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '008', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '120', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '027', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '126', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '020', 'per_video_db.shape = ', (66, 10, 90))\n"
     ]
    }
   ],
   "source": [
    "video_data_train1 = pickle.load(open('/Users/maojuntao/Downloads/project/data 2/shanghai_dataset_xyz_train.p','rb')) \n",
    "video_data_train1 = clip_xyz(video_data_train1)\n",
    "datadb1 = video_data_train1.copy()\n",
    "\n",
    "\n",
    "_video_db1,_video_db_future1,_video_db_future_input1 = get_data(datadb1,pick_user=False)\n",
    "    \n",
    "num_testing_sample = 1\n",
    "if cfg.input_mean_var:\n",
    "    encoder_input_data1 = get_gt_target_xyz(_video_db1[:-num_testing_sample,:,:])\n",
    "else:\n",
    "    encoder_input_data1 = _video_db1[:-num_testing_sample,:,:]\n",
    "decoder_target_data1 = get_gt_target_xyz(_video_db_future1)[:-num_testing_sample,:,:]\n",
    "# decoder_input_data = get_gt_target_xyz(_video_db_future_input)[:-num_testing_sample,-1,:][:,np.newaxis,:]\n",
    "decoder_input_data1 = get_gt_target_xyz(_video_db_future_input1)[:-num_testing_sample,0,:][:,np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6ae3652846d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10112, 10, 90)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10112, 1, 6)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-4f01492f4c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'all_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2593, 10, 90)\n",
      "(2593, 10, 90)\n",
      "(2593, 10, 90)\n"
     ]
    }
   ],
   "source": [
    "print(input.shape)\n",
    "print(_video_db.shape)\n",
    "print(_video_db_future_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(_video_db,open('video_db.p','wb'))\n",
    "pickle.dump(_video_db_future,open('video_db_future.p','wb'))\n",
    "pickle.dump(_video_db_future_input,open('video_db_future_input.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if not cfg.input_mean_var:\n",
    "        encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "        print(1)\n",
    "else:\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10112, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((2,10,90))\n",
    "b = np.ones((2,300,3))\n",
    "b[0,:,0] = a[0,:,:30].reshape(300)\n",
    "b[0,:,1] = a[0,:,30:60].reshape(300)\n",
    "b[0,:,2] = a[0,:,60:90].reshape(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(pretrained_weights = None):\n",
    "    inputs = Input(shape=(300,3))\n",
    "    conv1 = Conv1D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv1D(16, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling1D(2,data_format='channels_last')(conv1)\n",
    "    #\n",
    "    conv2 = Conv1D(32, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv1D(32, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling1D(2,data_format='channels_last')(conv2)\n",
    "    #\n",
    "    conv3 = Conv1D(64, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv1D(64, 3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    up1 = UpSampling1D(size=2 )(conv3)\n",
    "    up1 = concatenate([conv2,up1],axis=2)\n",
    "    conv4 = Conv1D(32, 3, activation='relu', padding='same')(up1)\n",
    "    conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv1D(32, 3, activation='relu', padding='same')(conv4)\n",
    "    #\n",
    "    up2 = UpSampling1D(size=2)(conv4)\n",
    "    up2 = concatenate([conv1,up2], axis=2)\n",
    "    conv5 = Conv1D(16,3, activation='relu', padding='same')(up2)\n",
    "    conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv1D(16, 3, activation='relu', padding='same')(conv5)\n",
    "    #\n",
    "    conv6 = Conv1D(6, 1, activation='relu',padding='same')(conv5)\n",
    "    pool3 = MaxPooling1D(30,data_format='channels_last')(conv6)\n",
    "    #conv6 = core.Reshape((6,patch_height*patch_width))(conv6)\n",
    "    #conv6 = core.Permute((2,1))(conv6)\n",
    "    ############\n",
    "    print('pool3',pool3.shape)\n",
    "    #decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(conv6)\n",
    "    b0,b1,b2,b3,b4,b5,b6,b7,b8,b9=tf.split(conv6,10,1)\n",
    "    print('b0',b0.shape)\n",
    "    b0=tf.reduce_mean(b0, axis=1,keep_dims=True)\n",
    "    b1=tf.reduce_mean(b1, axis=1,keep_dims=True)\n",
    "    b2=tf.reduce_mean(b2, axis=1,keep_dims=True)\n",
    "    b3=tf.reduce_mean(b3, axis=1,keep_dims=True)\n",
    "    b4=tf.reduce_mean(b4, axis=1,keep_dims=True)\n",
    "    b5=tf.reduce_mean(b5, axis=1,keep_dims=True)\n",
    "    b6=tf.reduce_mean(b6, axis=1,keep_dims=True)\n",
    "    b7=tf.reduce_mean(b7, axis=1,keep_dims=True)\n",
    "    b8=tf.reduce_mean(b8, axis=1,keep_dims=True)\n",
    "    b9=tf.reduce_mean(b9, axis=1,keep_dims=True)\n",
    "    \n",
    "    output=[]\n",
    "    output.append(b0)\n",
    "    output.append(b1)\n",
    "    output.append(b2)\n",
    "    output.append(b3)\n",
    "    output.append(b4)\n",
    "    output.append(b5)\n",
    "    output.append(b6)\n",
    "    output.append(b7)\n",
    "    output.append(b8)\n",
    "    output.append(b9)\n",
    "    '''\n",
    "    print('b0',b0.shape)\n",
    "    c=tf.stack([b0,b1,b2,b3,b4,b5,b6,b7,b8,b9],1)\n",
    "    print('tf.stack',c.shape)\n",
    "    '''\n",
    "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(output)\n",
    "    print(('1',encoder_inputs.shape))\n",
    "    print(('2',decoder_outputs.shape))\n",
    "    model = Model([inputs], pool3)\n",
    "\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import os \n",
    "import skimage.io as io \n",
    "import skimage.transform as trans \n",
    "import numpy as np \n",
    "from keras.models import * \n",
    "from keras.layers import * \n",
    "from keras.optimizers import * \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler \n",
    "from keras import backend as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pool3 (?, 10, 6)\n",
      "b0 (?, 30, 6)\n",
      "('1', TensorShape([Dimension(None), Dimension(None), Dimension(90)]))\n",
      "('2', TensorShape([Dimension(None), Dimension(10), Dimension(6)]))\n",
      "('_video_ind = ', '098', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '099', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '214', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '215', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '212', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '213', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '210', 'per_video_db.shape = ', (28, 10, 90))\n",
      "('_video_ind = ', '211', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '090', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '092', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '093', 'per_video_db.shape = ', (68, 10, 90))\n",
      "video 094 only has 17 seconds. skip...\n",
      "('_video_ind = ', '095', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '096', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '097', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '133', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '011', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 012 only has 18 seconds. skip...\n",
      "('_video_ind = ', '130', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '014', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '015', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '016', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '134', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '018', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '139', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '138', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '025', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '122', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '026', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '021', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '125', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '023', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '127', 'per_video_db.shape = ', (238, 10, 90))\n",
      "('_video_ind = ', '129', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '029', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 010 only has 18 seconds. skip...\n",
      "('_video_ind = ', '132', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '131', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '013', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '137', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '199', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '135', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '200', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '195', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '194', 'per_video_db.shape = ', (31, 10, 90))\n",
      "video 197 only has 18 seconds. skip...\n",
      "('_video_ind = ', '017', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '191', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '089', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '193', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '115', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '114', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '117', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '116', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '111', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '110', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '113', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '112', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '033', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '030', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '119', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '056', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '034', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '035', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '057', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '173', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '172', 'per_video_db.shape = ', (128, 10, 90))\n",
      "('_video_ind = ', '052', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '053', 'per_video_db.shape = ', (120, 10, 90))\n",
      "video 109 only has 17 seconds. skip...\n",
      "('_video_ind = ', '085', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '049', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '048', 'per_video_db.shape = ', (30, 10, 90))\n",
      "video 102 only has 11 seconds. skip...\n",
      "('_video_ind = ', '046', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '045', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '044', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '106', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '042', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '104', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '105', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '177', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '076', 'per_video_db.shape = ', (238, 10, 90))\n",
      "video 077 only has 19 seconds. skip...\n",
      "('_video_ind = ', '176', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '144', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '152', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '059', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '179', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '178', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '054', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '055', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '175', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '174', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '050', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '051', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '171', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '198', 'per_video_db.shape = ', (87, 10, 90))\n",
      "video 070 only has 19 seconds. skip...\n",
      "('_video_ind = ', '071', 'per_video_db.shape = ', (150, 10, 90))\n",
      "('_video_ind = ', '145', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '182', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '183', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '180', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '181', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '186', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '041', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '184', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '188', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '040', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '187', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '060', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '063', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '196', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '065', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '064', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '067', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '066', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '069', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '068', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '168', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '164', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '165', 'per_video_db.shape = ', (96, 10, 90))\n",
      "video 166 only has 19 seconds. skip...\n",
      "('_video_ind = ', '167', 'per_video_db.shape = ', (32, 10, 90))\n",
      "video 162 only has 19 seconds. skip...\n",
      "('_video_ind = ', '163', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '189', 'per_video_db.shape = ', (31, 10, 90))\n",
      "('_video_ind = ', '038', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '039', 'per_video_db.shape = ', (60, 10, 90))\n",
      "('_video_ind = ', '151', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '150', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '074', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '075', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '072', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '073', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '157', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '156', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '159', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '078', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '062', 'per_video_db.shape = ', (90, 10, 90))\n",
      "('_video_ind = ', '100', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '037', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '147', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '203', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '202', 'per_video_db.shape = ', (84, 10, 90))\n",
      "('_video_ind = ', '204', 'per_video_db.shape = ', (87, 10, 90))\n",
      "('_video_ind = ', '140', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '206', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '082', 'per_video_db.shape = ', (170, 10, 90))\n",
      "('_video_ind = ', '081', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '080', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '087', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '148', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '149', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '003', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '002', 'per_video_db.shape = ', (33, 10, 90))\n",
      "video 007 only has 18 seconds. skip...\n",
      "('_video_ind = ', '006', 'per_video_db.shape = ', (132, 10, 90))\n",
      "('_video_ind = ', '004', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '009', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '008', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '120', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '027', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '126', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '020', 'per_video_db.shape = ', (66, 10, 90))\n",
      "(10112, 300, 3)\n",
      "Train on 9100 samples, validate on 1012 samples\n",
      "Epoch 1/200\n",
      "9100/9100 [==============================] - 32s 3ms/step - loss: 0.1605 - val_loss: 0.1607\n",
      "Epoch 2/200\n",
      "9100/9100 [==============================] - 26s 3ms/step - loss: 0.1598 - val_loss: 0.1602\n",
      "Epoch 3/200\n",
      "9100/9100 [==============================] - 27s 3ms/step - loss: 0.1597 - val_loss: 0.1601\n",
      "Epoch 4/200\n",
      "9100/9100 [==============================] - 26s 3ms/step - loss: 0.1596 - val_loss: 0.1602\n",
      "Epoch 5/200\n",
      "9100/9100 [==============================] - 26s 3ms/step - loss: 0.1596 - val_loss: 0.1602\n",
      "Epoch 6/200\n",
      "9100/9100 [==============================] - 23s 3ms/step - loss: 0.1594 - val_loss: 0.1604\n",
      "Epoch 7/200\n",
      "9100/9100 [==============================] - 25s 3ms/step - loss: 0.1594 - val_loss: 0.1605\n",
      "Epoch 8/200\n",
      "9100/9100 [==============================] - 24s 3ms/step - loss: 0.1593 - val_loss: 0.1607\n",
      "Epoch 9/200\n",
      "9100/9100 [==============================] - 26s 3ms/step - loss: 0.1593 - val_loss: 0.1604\n",
      "Epoch 10/200\n",
      "9100/9100 [==============================] - 27s 3ms/step - loss: 0.1593 - val_loss: 0.1604\n",
      "Epoch 11/200\n",
      "9100/9100 [==============================] - 27s 3ms/step - loss: 0.1593 - val_loss: 0.1604\n",
      "Epoch 12/200\n",
      "9100/9100 [==============================] - 28s 3ms/step - loss: 0.1593 - val_loss: 0.1604\n",
      "Epoch 13/200\n",
      "9100/9100 [==============================] - 27s 3ms/step - loss: 0.1593 - val_loss: 0.1604\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-first part:\n",
    "seq2seq without teacher forcing\n",
    "-second part:\n",
    "seq2seq without teacher forcing, with others' future convLSTM\n",
    "concat states with decoder LSTM and then predict\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers import Lambda,Concatenate,Flatten,ConvLSTM2D\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "import sys,glob,io,random\n",
    "if '/Users/maojuntao/Downloads/project/' not in sys.path:\n",
    "    sys.path.insert(0, '/Users/maojuntao/Downloads/project/')\n",
    "from dataLayer import DataLayer\n",
    "import cost as costfunc\n",
    "from config import cfg\n",
    "from dataIO import clip_xyz\n",
    "from utility import reshape2second_stacks,get_data\n",
    "from utility import get_shuffle_index,shuffle_data,get_gt_target_xyz,get_gt_target_xyz_oth\n",
    "from utility import slice_layer\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #experiment = 1\n",
    "    batch_size = 32  # Batch size for training.\n",
    "    epochs = 200  # Number of epochs to train for.\n",
    "    latent_dim = 64  # Latent dimensionality of the encoding space.\n",
    "\n",
    "    fps = 30\n",
    "    num_encoder_tokens = 3*fps\n",
    "    num_decoder_tokens = 6\n",
    "    max_encoder_seq_length = cfg.running_length\n",
    "    max_decoder_seq_length = cfg.predict_step\n",
    "    # num_user = 48\n",
    "\n",
    "\n",
    "    model = get_unet()\n",
    "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    #model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "    #### ====================data====================\n",
    "    ## load data just as in Fov_seq2seq.py\n",
    "    # if cfg.use_xyz:\n",
    "    #     all_video_data = pickle.load(open('/scratch/cl2840/360video/data/new_exp_'+str(experiment)+'_xyz.p','rb'))\n",
    "    #     data_dim = 3\n",
    "    # all_video_data = clip_xyz(all_video_data)\n",
    "    # datadb = all_video_data.copy()\n",
    "    ##### data format 3or4--4\n",
    "    # video_data_train = pickle.load(open('/scratch/cl2840/360video/data/shanghai_dataset_xyz_train.p','rb'))    \n",
    "    #### format5\n",
    "    video_data_train = pickle.load(open('/Users/maojuntao/Downloads/project/data 2/shanghai_dataset_xyz_train.p','rb')) \n",
    "    video_data_train = clip_xyz(video_data_train)\n",
    "    datadb = video_data_train.copy()\n",
    "\n",
    "\n",
    "    _video_db,_video_db_future,_video_db_future_input = get_data(datadb,pick_user=False)\n",
    "    # total_num_samples = _video_db.shape[0]\n",
    "\n",
    "    if cfg.shuffle_data:\n",
    "        #shuffle the whole dataset\n",
    "        # index_shuf = get_shuffle_index(total_num_samples)\n",
    "        index_shuf = pickle.load(open('index_shuf'+'_exp'+str(experiment)+'.p','rb'))\n",
    "        _video_db = shuffle_data(index_shuf,_video_db)\n",
    "        _video_db_future = shuffle_data(index_shuf,_video_db_future)\n",
    "        _video_db_future_input = shuffle_data(index_shuf,_video_db_future_input)\n",
    "\n",
    "\n",
    "    # num_testing_sample = int(0.15*total_num_samples)#use last few as test\n",
    "    num_testing_sample = 1\n",
    "    if cfg.input_mean_var:\n",
    "        encoder_input_data = get_gt_target_xyz(_video_db[:-num_testing_sample,:,:])\n",
    "    else:\n",
    "        encoder_input_data = _video_db[:-num_testing_sample,:,:]\n",
    "    decoder_target_data = get_gt_target_xyz(_video_db_future)[:-num_testing_sample,:,:]\n",
    "    # decoder_input_data = get_gt_target_xyz(_video_db_future_input)[:-num_testing_sample,-1,:][:,np.newaxis,:]\n",
    "    decoder_input_data = get_gt_target_xyz(_video_db_future_input)[:-num_testing_sample,0,:][:,np.newaxis,:]\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Training====================\n",
    "    # model = load_model('fov_s2s_noteacherforcing_epoch21-0.0759.h5')\n",
    "    # tag = 'notFor_tanh_newdata_exp2_epoch'\n",
    "    # tag = 'fctar_seqseq_shanghai_traintest_split_predmeanvar_Aug9'\n",
    "    # tag = 'fctar_seqseq_shanghai_traintest_split_meanvarmeanvar_Aug9'\n",
    "    tag = 'fctar_seqseq_THU_predmeanvar_Sep5'\n",
    "    model_checkpoint = ModelCheckpoint(tag+'{epoch:02d}-{val_loss:.4f}.h5', monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                     patience=3, min_lr=1e-6)\n",
    "    stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "    b = np.ones((10112,300,3))\n",
    "    for i in range(10112):\n",
    "        b[i,:,0] = encoder_input_data[i,:,:30].reshape(300)\n",
    "        b[i,:,1] = encoder_input_data[i,:,30:60].reshape(300)\n",
    "        b[i,:,2] = encoder_input_data[i,:,60:90].reshape(300)\n",
    "    print(b.shape)\n",
    "\n",
    "    model.fit(b, decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True,initial_epoch=0,\n",
    "              callbacks=[model_checkpoint, reduce_lr, stopping])\n",
    "\n",
    "\n",
    "\n",
    "    ### ====================Testing====================\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('_video_ind = ', '146', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '153', 'per_video_db.shape = ', (96, 10, 90))\n",
      "video 155 only has 19 seconds. skip...\n",
      "('_video_ind = ', '154', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '158', 'per_video_db.shape = ', (96, 10, 90))\n",
      "('_video_ind = ', '190', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '079', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '128', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '058', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '136', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '032', 'per_video_db.shape = ', (99, 10, 90))\n",
      "('_video_ind = ', '019', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '031', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '118', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '170', 'per_video_db.shape = ', (32, 10, 90))\n",
      "('_video_ind = ', '160', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '028', 'per_video_db.shape = ', (66, 10, 90))\n",
      "('_video_ind = ', '209', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '185', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '201', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '088', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '142', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '143', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '141', 'per_video_db.shape = ', (136, 10, 90))\n",
      "('_video_ind = ', '083', 'per_video_db.shape = ', (68, 10, 90))\n",
      "('_video_ind = ', '208', 'per_video_db.shape = ', (58, 10, 90))\n",
      "('_video_ind = ', '192', 'per_video_db.shape = ', (62, 10, 90))\n",
      "('_video_ind = ', '024', 'per_video_db.shape = ', (132, 10, 90))\n",
      "video 001 only has 16 seconds. skip...\n",
      "('_video_ind = ', '123', 'per_video_db.shape = ', (68, 10, 90))\n",
      "video 124 only has 14 seconds. skip...\n",
      "('_video_ind = ', '169', 'per_video_db.shape = ', (64, 10, 90))\n",
      "('_video_ind = ', '005', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '022', 'per_video_db.shape = ', (33, 10, 90))\n",
      "('_video_ind = ', '047', 'per_video_db.shape = ', (30, 10, 90))\n",
      "('_video_ind = ', '103', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '101', 'per_video_db.shape = ', (34, 10, 90))\n",
      "('_video_ind = ', '043', 'per_video_db.shape = ', (120, 10, 90))\n",
      "('_video_ind = ', '161', 'per_video_db.shape = ', (93, 10, 90))\n",
      "('_video_ind = ', '205', 'per_video_db.shape = ', (29, 10, 90))\n",
      "('_video_ind = ', '091', 'per_video_db.shape = ', (102, 10, 90))\n",
      "('_video_ind = ', '121', 'per_video_db.shape = ', (68, 10, 90))\n",
      "111 (2593, 10, 90)\n",
      "Testing finished!\n"
     ]
    }
   ],
   "source": [
    " ##### data format 3or4--4\n",
    "    # video_data_test = pickle.load(open('/scratch/cl2840/360video/data/shanghai_dataset_xyz_test.p','rb'))\n",
    "    ### data format 5\n",
    "video_data_test = pickle.load(open('/Users/maojuntao/Downloads/project/data 2/shanghai_dataset_xyz_test.p','rb'))\n",
    "\n",
    "video_data_test = clip_xyz(video_data_test)\n",
    "datadb = video_data_test.copy()   ##arrange the test data\n",
    "_video_db,_video_db_future,_video_db_future_input = get_data(datadb,pick_user=False)\n",
    "\n",
    "if cfg.input_mean_var:\n",
    "    video_db = get_gt_target_xyz(_video_db)\n",
    "\n",
    "        # model = load_model('fov_s2s_noteacherforcing.h5')\n",
    "        # model = load_model('fov_s2s_noteacherforcing_epoch74-0.0087.h5')\n",
    "        # model = load_model('notFor_tanh_newdata_epoch18-0.0761.h5')\n",
    "        # model = load_model('notFor_tanh_newdata_exp2_epoch200-0.0302.h5')\n",
    "c=np.ones((2593,300,3))\n",
    "def decode_sequence_fov(input_seq):\n",
    "            # Encode the input as state vectors.\n",
    "    '''\n",
    "    last_location = input_seq[0,-1,:][np.newaxis,np.newaxis,:]\n",
    "    if not cfg.input_mean_var:\n",
    "        last_mu_var = get_gt_target_xyz(last_location)\n",
    "    last_mu_var = last_location\n",
    "    else:\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    decoded_sentence = model.predict(input_seq)\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "gt_sentence_list = []\n",
    "decoded_sentence_list = []\n",
    "print('111',_video_db.shape)\n",
    "for i in range(_video_db.shape[0]):\n",
    "    c[i,:,0] = _video_db[i,:,:30].reshape(300)\n",
    "    c[i,:,1] = _video_db[i,:,30:60].reshape(300)\n",
    "    c[i,:,2] = _video_db[i,:,60:90].reshape(300)\n",
    "        # for seq_index in range(total_num_samples-num_testing_sample,total_num_samples):\n",
    "for seq_index in range(c.shape[0]):\n",
    "            # Take one sequence (part of the training set)\n",
    "            # for trying out decoding.\n",
    "    input_seq = c[seq_index: seq_index + 1,:,:]\n",
    "\n",
    "    decoded_sentence = decode_sequence_fov(input_seq)\n",
    "    decoded_sentence_list+=[decoded_sentence]\n",
    "    gt_sentence = _video_db_future[seq_index: seq_index + 1,:,:]\n",
    "    gt_sentence_list+=[gt_sentence]\n",
    "    decoder_target = get_gt_target_xyz(gt_sentence)\n",
    "            # print('-')\n",
    "            # print('Decoded sentence - decoder_target:', np.squeeze(np.array(decoded_sentence))[:,:3]-np.squeeze(decoder_target)[:,:3])\n",
    "\n",
    "pickle.dump(decoded_sentence_list,open('decoded_sentence'+tag+'.p','wb'))\n",
    "pickle.dump(gt_sentence_list,open('gt_sentence_list'+tag+'.p','wb'))\n",
    "print('Testing finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

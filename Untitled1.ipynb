{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named torch.utils.data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f008c912e367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named torch.utils.data"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Function, Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from skimage import transform\n",
    "import glob\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SINGLE_CONV CLASS\n",
    "class single_conv(nn.Module):\n",
    "        '''(conv => BN => ReLU) '''\n",
    "        def __init__(self, in_ch, out_ch):\n",
    "            super(single_conv, self).__init__()\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_ch,out_ch,3,padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "# DEFINE DOWN CLASS\n",
    "class down(nn.Module):\n",
    "                        def __init__(self, in_ch, out_ch):\n",
    "                            super(down, self).__init__()\n",
    "                            self.down = nn.MaxPool2d(2,stride=2)      # use nn.MaxPool2d( )\n",
    "                            self.conv = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "                        def forward(self, x):\n",
    "                            x = self.down(x)\n",
    "                            x = self.conv(x)\n",
    "                            return x\n",
    "    \n",
    "\n",
    "# DEFINE UP CLASS\n",
    "# Note that this class will not only upsample x1, but also concatenate up-sampled x1 with x2 to generate the final output\n",
    "\n",
    "class up(nn.Module):\n",
    "        def __init__(self, in_ch, out_ch):\n",
    "            super(up, self).__init__()       \n",
    "            self.up = nn.Upsample(scale_factor=2, mode='nearest') # use nn.Upsample( )\n",
    "            self.conv = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "\n",
    "        def forward(self, x1, x2):\n",
    "        # This part is tricky, so we provide it for you\n",
    "        # First we upsample x1\n",
    "            x1 = self.up(x1)\n",
    "            \n",
    "        # Notice that x2 and x1 may not have the same spatial size. \n",
    "        # This is because when you downsample old_x2(say 25 by 25), you will get x1(12 by 12)   \n",
    "        # Then you perform upsample to x1, you will get new_x1(24 by 24)\n",
    "        # You should pad a new row and column so that new_x1 and x2 have the same size.\n",
    "        \n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "            x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                            diffY // 2, diffY - diffY//2))\n",
    "\n",
    "        # Now we concatenat x2 channels with x1 channels\n",
    "            x = torch.cat([x2, x1], dim=1)\n",
    "            x = self.conv(x)\n",
    "            return x\n",
    "\n",
    "# DEFINE OUTCONV CLASS\n",
    "class outconv(nn.Module):\n",
    "        def __init__(self, in_ch, out_ch):\n",
    "            super(outconv, self).__init__()\n",
    "            self.conv = nn.Conv2d(in_ch,out_ch,3,padding=1) # Use nn.Conv2D( ) since we do not need to do batch norm and relu at this layer\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
